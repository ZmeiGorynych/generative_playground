{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0.4.0a0+408c84d\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "import torch\n",
    "print(torch.cuda.is_available(), torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0596e+34\n",
      " 4.5652e-41\n",
      " 8.0474e-38\n",
      " 0.0000e+00\n",
      " 4.4842e-44\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n",
      "\n",
      " 1.0596e+34\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-41 *\n",
      "  4.5652\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-38 *\n",
      "  8.0474\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-44 *\n",
      "  4.4842\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(5,1)\n",
    "print(a)\n",
    "for a_ in a:\n",
    "    print(a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function generating warp matrix from vector of numbers between 0 and 1\n",
    "# reconcile new version against old\n",
    "from reshape import WarpMatrix,WarpMatrixOld\n",
    "from torch.autograd import Variable\n",
    "from utils import to_gpu\n",
    "import torch\n",
    "\n",
    "w1=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "w2=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "w2.data = w1.data\n",
    "wmat1 = WarpMatrix.apply(w1)\n",
    "loss1 = wmat1.sum(0)[1]\n",
    "loss1.backward()\n",
    "wmat2 = WarpMatrixOld.apply(w2)\n",
    "loss2 = wmat2.sum(0)[1]\n",
    "loss2.backward()\n",
    "\n",
    "print((wmat1-wmat2).abs().max(), \n",
    "      (loss1-loss2).abs().max(),\n",
    "      (w1.grad-w2.grad).abs().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.2711  0.5778  0.1511  0.0000  0.0000\n",
      "  0.0000  0.0000  0.7750  0.2250  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0100  0.8166\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.6888  0.3112  0.0000  0.0000  0.0000\n",
      "  0.0000  0.3057  0.5969  0.0974  0.0000\n",
      "  0.0000  0.0000  0.0000  0.8734  0.1266\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0110\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 2x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now try feeding it batch data\n",
    "w1=Variable(to_gpu(torch.FloatTensor(2,5,1).uniform_()), requires_grad = True)\n",
    "wmat1 = WarpMatrix.apply(w1)\n",
    "loss1 = wmat1.sum(0).sum(0)[1]\n",
    "loss1.backward()\n",
    "print(wmat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# profile WarpMatrix autograd\n",
    "# class dummy:\n",
    "#     pass\n",
    "\n",
    "# ctx = dummy()\n",
    "# w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "# wmat = WarpMatrix.forward(ctx,w.data)\n",
    "# %lprun -f WarpMatrix.backward WarpMatrix.backward(ctx,Variable(wmat))\n",
    "#print(wmat,pseudo_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's test the whole fitted-weights-compression transform as a layer\n",
    "from reshape import FittedWarp\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "true_w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "warp = FittedWarp(true_w.shape)\n",
    "x = Variable(to_gpu(torch.randn(2,10,warp.input_shape[1])))\n",
    "out = warp(x)\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "#print(out, loss,warp.w.grad)\n",
    "# a =w.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from visualize import make_dot\n",
    "# make_dot(loss)\n",
    "# wait for feedback from the function author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the 'true' model\n",
    "true_w=Variable(to_gpu(torch.FloatTensor(5,1).uniform_()), requires_grad = True)\n",
    "true_warp = to_gpu(FittedWarp(true_w.shape))\n",
    "true_warp.w.data = true_w.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "# testing a pytorch-style data loader\n",
    "from data_sources import DatasetFromModel\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = DatasetFromModel(100, 100, true_warp)\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True)\n",
    "out = next(dataloader.__iter__())\n",
    "print(out[0].shape,len(out[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "epoch  0\n",
      "1.0\n",
      "0.27980769172310826 100\n",
      "we're improving! 0.20899468660354614\n",
      "successfully saved model\n",
      "epoch  1\n",
      "0.9772372209558107\n",
      "0.1668003487586975 100\n",
      "we're improving! 0.13152430951595306\n",
      "successfully saved model\n",
      "epoch  2\n",
      "0.954992586021436\n",
      "0.10315112724900245 100\n",
      "we're improving! 0.08402388542890549\n",
      "successfully saved model\n",
      "epoch  3\n",
      "0.933254300796991\n",
      "0.06994044698774815 100\n",
      "we're improving! 0.057797107845544815\n",
      "successfully saved model\n",
      "epoch  4\n",
      "0.9120108393559098\n",
      "0.051619466096162796 100\n",
      "we're improving! 0.04682411253452301\n",
      "successfully saved model\n",
      "epoch  5\n",
      "0.8912509381337456\n",
      "0.04433164833113551 100\n",
      "we're improving! 0.042893726378679276\n",
      "successfully saved model\n",
      "epoch  6\n",
      "0.8709635899560807\n",
      "0.0415359453111887 100\n",
      "we're improving! 0.03970501571893692\n",
      "successfully saved model\n",
      "epoch  7\n",
      "0.8511380382023764\n",
      "0.038822384290397166 100\n",
      "epoch  8\n",
      "0.831763771102671\n",
      "0.038466623909771444 100\n",
      "we're improving! 0.03863820061087608\n",
      "successfully saved model\n",
      "epoch  9\n",
      "0.8128305161640993\n",
      "0.03653276801109314 100\n",
      "we're improving! 0.03861098363995552\n",
      "successfully saved model\n",
      "epoch  10\n",
      "0.7943282347242815\n",
      "0.03737517431378365 100\n",
      "we're improving! 0.035878706723451614\n",
      "successfully saved model\n",
      "epoch  11\n",
      "0.7762471166286917\n",
      "0.03938859699293971 100\n",
      "epoch  12\n",
      "0.7585775750291838\n",
      "0.037622011974453924 100\n",
      "epoch  13\n",
      "0.7413102413009175\n",
      "0.0365511747263372 100\n",
      "epoch  14\n",
      "0.72443596007499\n",
      "0.037647134866565464 100\n",
      "epoch  15\n",
      "0.7079457843841379\n",
      "0.038025334738194944 100\n",
      "epoch  16\n",
      "0.6918309709189365\n",
      "0.0374824682995677 100\n",
      "epoch  17\n",
      "0.6760829753919818\n",
      "0.03847481966018677 100\n",
      "epoch  18\n",
      "0.660693448007596\n",
      "0.03871361002326012 100\n",
      "epoch  19\n",
      "0.6456542290346555\n",
      "0.03798780167475343 100\n",
      "epoch  20\n",
      "0.6309573444801932\n",
      "0.03840636972337961 100\n",
      "epoch  21\n",
      "0.6165950018614822\n",
      "0.03824813673272729 100\n",
      "epoch  22\n",
      "0.6025595860743578\n",
      "0.03775913713499904 100\n",
      "epoch  23\n",
      "0.5888436553555889\n",
      "0.03897130830213427 100\n",
      "epoch  24\n",
      "0.5754399373371569\n",
      "0.03782751854509115 100\n",
      "epoch  25\n",
      "0.5623413251903491\n",
      "0.038483414780348536 100\n",
      "epoch  26\n",
      "0.5495408738576245\n",
      "0.038010914251208304 100\n",
      "epoch  27\n",
      "0.5370317963702527\n",
      "0.03804073553532362 100\n",
      "epoch  28\n",
      "0.5248074602497725\n",
      "0.03775714736431837 100\n",
      "epoch  29\n",
      "0.5128613839913649\n",
      "0.03657567281275988 100\n",
      "epoch  30\n",
      "0.5011872336272724\n",
      "0.038554874695837495 100\n",
      "epoch  31\n",
      "0.4897788193684462\n",
      "0.037848420590162274 100\n",
      "epoch  32\n",
      "0.47863009232263837\n",
      "0.037921078968793154 100\n",
      "epoch  33\n",
      "0.4677351412871982\n",
      "0.03780351400375366 100\n",
      "epoch  34\n",
      "0.457088189614875\n",
      "0.03728736719116568 100\n",
      "epoch  35\n",
      "0.44668359215096315\n",
      "0.03795359805226326 100\n",
      "epoch  36\n",
      "0.436515832240166\n",
      "0.038653395865112544 100\n",
      "epoch  37\n",
      "0.42657951880159267\n",
      "0.03588655833154917 100\n",
      "epoch  38\n",
      "0.4168693834703354\n",
      "0.038087549321353435 100\n",
      "epoch  39\n",
      "0.40738027780411273\n",
      "0.037468295469880106 100\n",
      "epoch  40\n",
      "0.39810717055349726\n",
      "0.03738132992759347 100\n",
      "epoch  41\n",
      "0.3890451449942806\n",
      "0.038833681177347895 100\n",
      "epoch  42\n",
      "0.3801893963205612\n",
      "0.038526082262396816 100\n",
      "epoch  43\n",
      "0.37153522909717257\n",
      "0.037393719404935834 100\n",
      "epoch  44\n",
      "0.36307805477010135\n",
      "0.036676321644335985 100\n",
      "we're improving! 0.035365890711545944\n",
      "successfully saved model\n",
      "epoch  45\n",
      "0.35481338923357547\n",
      "0.038894505836069584 100\n",
      "epoch  46\n",
      "0.3467368504525316\n",
      "0.03968696378171444 100\n",
      "epoch  47\n",
      "0.3388441561392026\n",
      "0.03769537707790732 100\n",
      "epoch  48\n",
      "0.3311311214825911\n",
      "0.038546905182302 100\n",
      "epoch  49\n",
      "0.32359365692962827\n",
      "0.0379422196559608 100\n",
      "epoch  50\n",
      "0.31622776601683794\n",
      "0.03817877957597375 100\n",
      "epoch  51\n",
      "0.30902954325135906\n",
      "0.03754248645156622 100\n",
      "epoch  52\n",
      "0.3019951720402016\n",
      "0.0380601199157536 100\n",
      "epoch  53\n",
      "0.2951209226666386\n",
      "0.037479738183319566 100\n",
      "epoch  54\n",
      "0.28840315031266056\n",
      "0.03834056621417403 100\n",
      "epoch  55\n",
      "0.28183829312644537\n",
      "0.03747114961966872 100\n",
      "epoch  56\n",
      "0.2754228703338166\n",
      "0.038005293365567926 100\n",
      "epoch  57\n",
      "0.2691534803926916\n",
      "0.0401421108096838 100\n",
      "epoch  58\n",
      "0.26302679918953825\n",
      "0.03719352511689067 100\n",
      "epoch  59\n",
      "0.2570395782768864\n",
      "0.03856649154797196 100\n",
      "epoch  60\n",
      "0.251188643150958\n",
      "0.03699686193838716 100\n",
      "epoch  61\n",
      "0.24547089156850305\n",
      "0.0381320733204484 100\n",
      "epoch  62\n",
      "0.23988329190194907\n",
      "0.03830982832238078 100\n",
      "we're improving! 0.0352155901491642\n",
      "successfully saved model\n",
      "epoch  63\n",
      "0.23442288153199223\n",
      "0.03676572080701589 100\n",
      "epoch  64\n",
      "0.2290867652767773\n",
      "0.03899716166779399 100\n",
      "epoch  65\n",
      "0.22387211385683395\n",
      "0.03652965374290943 100\n",
      "epoch  66\n",
      "0.21877616239495526\n",
      "0.037906057052314285 100\n",
      "epoch  67\n",
      "0.2137962089502232\n",
      "0.0372084722109139 100\n",
      "epoch  68\n",
      "0.20892961308540392\n",
      "0.03773673553019762 100\n",
      "epoch  69\n",
      "0.20417379446695297\n",
      "0.038687964007258414 100\n",
      "we're improving! 0.035037070512771606\n",
      "successfully saved model\n",
      "epoch  70\n",
      "0.199526231496888\n",
      "0.03724387599155307 100\n",
      "epoch  71\n",
      "0.19498445997580455\n",
      "0.03778928937390447 100\n",
      "epoch  72\n",
      "0.19054607179632474\n",
      "0.03879595877602696 100\n",
      "epoch  73\n",
      "0.18620871366628677\n",
      "0.037611975688487294 100\n",
      "epoch  74\n",
      "0.18197008586099836\n",
      "0.038898320458829405 100\n",
      "epoch  75\n",
      "0.1778279410038923\n",
      "0.03840022627264261 100\n",
      "epoch  76\n",
      "0.17378008287493754\n",
      "0.03870857546105981 100\n",
      "epoch  77\n",
      "0.16982436524617445\n",
      "0.037199672013521194 100\n",
      "epoch  78\n",
      "0.16595869074375605\n",
      "0.036586525179445745 100\n",
      "epoch  79\n",
      "0.162181009735893\n",
      "0.037307809256017205 100\n",
      "epoch  80\n",
      "0.15848931924611134\n",
      "0.03775029417127371 100\n",
      "epoch  81\n",
      "0.15488166189124813\n",
      "0.0373580945469439 100\n",
      "epoch  82\n",
      "0.15135612484362085\n",
      "0.037771180775016545 100\n",
      "epoch  83\n",
      "0.14791083881682077\n",
      "0.03889624439179897 100\n",
      "epoch  84\n",
      "0.14454397707459277\n",
      "0.03756717013195157 100\n",
      "epoch  85\n",
      "0.14125375446227545\n",
      "0.037656799536198375 100\n",
      "epoch  86\n",
      "0.1380384264602885\n",
      "0.03734109595417976 100\n",
      "epoch  87\n",
      "0.13489628825916536\n",
      "0.03912286594510078 100\n",
      "epoch  88\n",
      "0.1318256738556407\n",
      "0.03815520109608769 100\n",
      "epoch  89\n",
      "0.1288249551693134\n",
      "0.038523910921067 100\n",
      "epoch  90\n",
      "0.12589254117941673\n",
      "0.038011374901980165 100\n",
      "epoch  91\n",
      "0.12302687708123815\n",
      "0.037891433015465735 100\n",
      "epoch  92\n",
      "0.12022644346174129\n",
      "0.03891361614689231 100\n",
      "epoch  93\n",
      "0.11748975549395295\n",
      "0.03719117989763618 100\n",
      "epoch  94\n",
      "0.1148153621496883\n",
      "0.038804267216473814 100\n",
      "epoch  95\n",
      "0.11220184543019636\n",
      "0.03790981635451317 100\n",
      "epoch  96\n",
      "0.10964781961431852\n",
      "0.03742963884025812 100\n",
      "epoch  97\n",
      "0.10715193052376065\n",
      "0.039246521554887295 100\n",
      "epoch  98\n",
      "0.10471285480508996\n",
      "0.03848063852638006 100\n",
      "epoch  99\n",
      "0.10232929922807542\n",
      "0.038259135261178016 100\n",
      "epoch  100\n",
      "0.1\n",
      "0.0377261739037931 100\n",
      "epoch  101\n",
      "0.1\n",
      "0.03934815902262926 100\n",
      "epoch  102\n",
      "0.1\n",
      "0.03717687761411071 100\n",
      "epoch  103\n",
      "0.1\n",
      "0.038691519256681205 100\n",
      "epoch  104\n",
      "0.1\n",
      "0.03830978773534298 100\n",
      "epoch  105\n",
      "0.1\n",
      "0.03850434673950076 100\n",
      "epoch  106\n",
      "0.1\n",
      "0.03660073079168796 100\n",
      "epoch  107\n",
      "0.1\n",
      "0.03794091081246734 100\n",
      "epoch  108\n",
      "0.1\n",
      "0.038619289118796585 100\n",
      "epoch  109\n",
      "0.1\n",
      "0.038001865167170766 100\n",
      "epoch  110\n",
      "0.1\n",
      "0.03816151874139905 100\n",
      "epoch  111\n",
      "0.1\n",
      "0.037498888690024615 100\n",
      "epoch  112\n",
      "0.1\n",
      "0.03632024517282843 100\n",
      "epoch  113\n",
      "0.1\n",
      "0.03974506441503763 100\n",
      "epoch  114\n",
      "0.1\n",
      "0.0384006330743432 100\n",
      "epoch  115\n",
      "0.1\n",
      "0.038617103826254606 100\n",
      "epoch  116\n",
      "0.1\n",
      "0.03771190077066421 100\n",
      "epoch  117\n",
      "0.1\n",
      "0.03844741987064481 100\n",
      "epoch  118\n",
      "0.1\n",
      "0.03745023027062416 100\n",
      "epoch  119\n",
      "0.1\n",
      "0.03828270450234413 100\n",
      "epoch  120\n",
      "0.1\n",
      "0.038000705558806656 100\n",
      "epoch  121\n",
      "0.1\n",
      "0.038685517255216836 100\n",
      "epoch  122\n",
      "0.1\n",
      "0.03963504893705249 100\n",
      "epoch  123\n",
      "0.1\n",
      "0.03714948140084744 100\n",
      "epoch  124\n",
      "0.1\n",
      "0.0368465818092227 100\n",
      "epoch  125\n",
      "0.1\n",
      "0.037759314179420474 100\n",
      "epoch  126\n",
      "0.1\n",
      "0.03848970493301749 100\n",
      "epoch  127\n",
      "0.1\n",
      "0.037892847638577226 100\n",
      "epoch  128\n",
      "0.1\n",
      "0.0381270176358521 100\n",
      "epoch  129\n",
      "0.1\n",
      "0.03823468945920468 100\n",
      "epoch  130\n",
      "0.1\n",
      "0.03841535855084657 100\n",
      "epoch  131\n",
      "0.1\n",
      "0.03921222051605582 100\n",
      "epoch  132\n",
      "0.1\n",
      "0.03866551773622632 100\n",
      "epoch  133\n",
      "0.1\n",
      "0.038116464577615264 100\n",
      "epoch  134\n",
      "0.1\n",
      "0.03795004390180111 100\n",
      "epoch  135\n",
      "0.1\n",
      "0.03772764790803194 100\n",
      "epoch  136\n",
      "0.1\n",
      "0.03777181366458535 100\n",
      "epoch  137\n",
      "0.1\n",
      "0.038122653886675836 100\n",
      "epoch  138\n",
      "0.1\n",
      "0.038734580613672735 100\n",
      "epoch  139\n",
      "0.1\n",
      "0.0383641223423183 100\n",
      "epoch  140\n",
      "0.1\n",
      "0.03879502521827817 100\n",
      "epoch  141\n",
      "0.1\n",
      "0.03753935504704714 100\n",
      "epoch  142\n",
      "0.1\n",
      "0.037163832206279036 100\n",
      "epoch  143\n",
      "0.1\n",
      "0.038068367671221494 100\n",
      "epoch  144\n",
      "0.1\n",
      "0.03810930794104934 100\n",
      "epoch  145\n",
      "0.1\n",
      "0.038025906626135114 100\n",
      "epoch  146\n",
      "0.1\n",
      "0.037651599552482366 100\n",
      "epoch  147\n",
      "0.1\n",
      "0.03636161893606186 100\n",
      "epoch  148\n",
      "0.1\n",
      "0.03752013884484768 100\n",
      "epoch  149\n",
      "0.1\n",
      "0.03806135576218367 100\n",
      "epoch  150\n",
      "0.1\n",
      "0.03748609276488423 100\n",
      "epoch  151\n",
      "0.1\n",
      "0.03781040104106068 100\n",
      "epoch  152\n",
      "0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0388610858283937 100\n",
      "epoch  153\n",
      "0.1\n",
      "0.03797234278172255 100\n",
      "epoch  154\n",
      "0.1\n",
      "0.037454910669475795 100\n",
      "epoch  155\n",
      "0.1\n",
      "0.03815012799575925 100\n",
      "epoch  156\n",
      "0.1\n",
      "0.03750163091346621 100\n",
      "epoch  157\n",
      "0.1\n",
      "0.038591526988893746 100\n",
      "epoch  158\n",
      "0.1\n",
      "0.03849819887429476 100\n",
      "epoch  159\n",
      "0.1\n",
      "0.03876106645911932 100\n",
      "epoch  160\n",
      "0.1\n",
      "0.03780947322025895 100\n",
      "epoch  161\n",
      "0.1\n",
      "0.03713305700570345 100\n",
      "epoch  162\n",
      "0.1\n",
      "0.03837002070620656 100\n",
      "epoch  163\n",
      "0.1\n",
      "0.03734414931386709 100\n",
      "epoch  164\n",
      "0.1\n",
      "0.037725348714739086 100\n",
      "epoch  165\n",
      "0.1\n",
      "0.03830823574215174 100\n",
      "we're improving! 0.03492527827620506\n",
      "successfully saved model\n",
      "epoch  166\n",
      "0.1\n",
      "0.03721010403707623 100\n",
      "epoch  167\n",
      "0.1\n",
      "0.03754899239167571 100\n",
      "epoch  168\n",
      "0.1\n",
      "0.03779546959325671 100\n",
      "epoch  169\n",
      "0.1\n",
      "0.03848697278648615 100\n",
      "epoch  170\n",
      "0.1\n",
      "0.037117636036127805 100\n",
      "epoch  171\n",
      "0.1\n",
      "0.0387632642313838 100\n",
      "epoch  172\n",
      "0.1\n",
      "0.0381763762421906 100\n",
      "epoch  173\n",
      "0.1\n",
      "0.0377992743998766 100\n",
      "epoch  174\n",
      "0.1\n",
      "0.038511647544801235 100\n",
      "epoch  175\n",
      "0.1\n",
      "0.03756234327331185 100\n",
      "epoch  176\n",
      "0.1\n",
      "0.03749858096241951 100\n",
      "epoch  177\n",
      "0.1\n",
      "0.03813585190102458 100\n",
      "epoch  178\n",
      "0.1\n",
      "0.03724767558276653 100\n",
      "epoch  179\n",
      "0.1\n",
      "0.037988653872162104 100\n",
      "epoch  180\n",
      "0.1\n",
      "0.03869181120768189 100\n",
      "epoch  181\n",
      "0.1\n",
      "0.037156707514077424 100\n",
      "epoch  182\n",
      "0.1\n",
      "0.037369759008288386 100\n",
      "epoch  183\n",
      "0.1\n",
      "0.04028218729421496 100\n",
      "epoch  184\n",
      "0.1\n",
      "0.0380306782014668 100\n",
      "epoch  185\n",
      "0.1\n",
      "0.039341971818357704 100\n",
      "epoch  186\n",
      "0.1\n",
      "0.03814351467415691 100\n",
      "epoch  187\n",
      "0.1\n",
      "0.038075099270790815 100\n",
      "epoch  188\n",
      "0.1\n",
      "0.036369918957352636 100\n",
      "epoch  189\n",
      "0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-88275a4f6c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         save_path=save_path)\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmy_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#%lprun -f fit my_fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#print(model.w, true_warp.w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-88275a4f6c65>\u001b[0m in \u001b[0;36mmy_fit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         save_path=save_path)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmy_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/basic_pytorch/fit.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_gen, valid_gen, model, optimizer, scheduler, epochs, criterion, save_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mloss_\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 83\u001b[0;31m         variables, grad_variables, retain_graph, create_graph)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now let's fit those weights\n",
    "from torch.optim import lr_scheduler\n",
    "from data_sources import data_gen\n",
    "from torch.autograd import Variable\n",
    "from fit import fit\n",
    "from models import FittedWarpWithConvolution, FittedWarp\n",
    "from utils import to_gpu\n",
    "\n",
    "model_type = FittedWarpWithConvolution\n",
    "\n",
    "# 'true' model\n",
    "true_w=Variable(to_gpu(torch.randn(100,1)), requires_grad = True)\n",
    "true_warp = to_gpu(model_type(w=true_w))\n",
    "# create a model with random weights, for training\n",
    "model = to_gpu(model_type(true_w.shape))\n",
    "\n",
    "# datasets generated from 'true' model\n",
    "train_dataset = DatasetFromModel(100, 100, true_warp)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1,\n",
    "                        shuffle=True)\n",
    "\n",
    "valid_dataset = DatasetFromModel(1000, 2, true_warp)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2,\n",
    "                        shuffle=True)\n",
    "\n",
    "# and all the other bits we need\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def lr_fun(epoch):\n",
    "    mult = 0.1**(min(epoch,100)/100)\n",
    "    print(mult)\n",
    "    return(mult)\n",
    "    \n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_fun, last_epoch=-1)\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 1000\n",
    "save_path = 'test.mdl'\n",
    "\n",
    "\n",
    "def my_fit():\n",
    "    fit(train_gen = train_dataloader,\n",
    "        valid_gen = valid_dataloader,\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler,\n",
    "        epochs = epochs,\n",
    "        criterion = criterion,\n",
    "        save_path=save_path)\n",
    "\n",
    "my_fit()\n",
    "#%lprun -f fit my_fit()\n",
    "#print(model.w, true_warp.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradchek takes a tuple of tensor as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),)\n",
    "test = gradcheck(WarpMatrix.apply, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "class MyFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = torch.sign(input)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # saved tensors - tuple of tensors, so we need get first\n",
    "        input, = ctx.saved_variables\n",
    "#         grad_output[input.ge(1)] = 0\n",
    "#         grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# usage\n",
    "x = torch.autograd.Variable(torch.randn(10, 20), requires_grad = True)\n",
    "y = MyFunction.apply(x)\n",
    "# or\n",
    "# my_func = MyFunction.apply\n",
    "# y = my_func(x)\n",
    "loss = y.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(10, 20), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 3), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 3), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "for step in range(5):\n",
    "    pred = F.sigmoid(x @ w1)\n",
    "    pred = F.sigmoid(pred @ w2)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # calculate new gradients\n",
    "    loss.backward()\n",
    "    # apply new gradients\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "first_counter = torch.Tensor([0])\n",
    "second_counter = torch.Tensor([10])\n",
    "some_value = torch.Tensor(15)\n",
    "\n",
    "while (first_counter < second_counter)[0]:\n",
    "    first_counter += 2\n",
    "    second_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next([1].iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20, 64, 5),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Example of using Sequential with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "#output = model(some_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(12, 24, kernel_size=3, padding=1, stride=1),\n",
    "        )\n",
    "        self.second_extractor = nn.Conv2d(\n",
    "            24, 36, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.second_extractor(x)\n",
    "        # note that we may call same layer twice or mode\n",
    "        x = self.second_extractor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = torch.sign(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # saved tensors - tuple of tensors, so we need get first\n",
    "        input, = ctx.saved_variables\n",
    "        grad_output[input.ge(1)] = 0\n",
    "        grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# usage\n",
    "x = torch.randn(10, 20)\n",
    "y = MyFunction.apply(x)\n",
    "# or\n",
    "my_func = MyFunction.apply\n",
    "print(MyFunction)\n",
    "y = my_func(x)\n",
    "\n",
    "\n",
    "# and if we want to use inside nn.Module\n",
    "class MyFunctionModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MyFunction.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "### tensor example\n",
    "x_cpu = torch.randn(10, 20)\n",
    "w_cpu = torch.randn(20, 10)\n",
    "# direct transfer to the GPU\n",
    "x_gpu = x_cpu.cuda()\n",
    "w_gpu = w_cpu.cuda()\n",
    "result_gpu = x_gpu @ w_gpu\n",
    "# get back from GPU to CPU\n",
    "result_cpu = result_gpu.cpu()\n",
    "\n",
    "### model example\n",
    "model = model.cuda()\n",
    "# train step\n",
    "inputs = Variable(inputs.cuda())\n",
    "outputs = model(inputs)\n",
    "# get back from GPU to CPU\n",
    "outputs = outputs.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# check is cuda enabled\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# set required device\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# work with some required cuda device\n",
    "with torch.cuda.device(1):\n",
    "    # allocates a tensor on GPU 1\n",
    "    a = torch.cuda.FloatTensor(1)\n",
    "    assert a.get_device() == 1\n",
    "\n",
    "    # but you still can manually assign tensor to required device\n",
    "    d = torch.randn(2).cuda(2)\n",
    "    assert d.get_device() == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# new way with `init` module\n",
    "w = torch.Tensor(3, 5)\n",
    "torch.nn.init.normal(w)\n",
    "# work for Variables also\n",
    "w2 = Variable(w)\n",
    "torch.nn.init.normal(w2)\n",
    "# old styled direct access to tensors data attribute\n",
    "w2.data.normal_()\n",
    "\n",
    "# example for some module\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# for loop approach with direct access\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scheduler example\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    train()\n",
    "    validate()\n",
    "\n",
    "# Train flag can be updated with boolean\n",
    "# to disable dropout and batch norm learning\n",
    "model.train(True)\n",
    "# execute train step\n",
    "model.train(False)\n",
    "# run inference step\n",
    "\n",
    "# CPU seed\n",
    "torch.manual_seed(42)\n",
    "# GPU seed\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn as nn\n",
    "save_path = 'test.mdl'\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Sequential (\n",
    "#   (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu1): ReLU ()\n",
    "#   (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu2): ReLU ()\n",
    "# )\n",
    "\n",
    "# save/load only the model parameters(prefered solution)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# save whole model\n",
    "torch.save(model, save_path)\n",
    "model = torch.load(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    return x.cuda()\n",
    "\n",
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    pass\n",
    "\n",
    "class Net(nn.Module):\n",
    "    pass\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "fit_dataset = ImagesDataset(path_to_fit_images)\n",
    "fit_data_loader = torch.utils.data.DataLoader(fitdataset, batch_size=10)\n",
    "\n",
    "valid_dataset = ImagesDataset(path_to_valid_images)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=10)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    lr_scheduler.step()\n",
    "    for inputs, labels in fit_data_loader:\n",
    "        inputs = Variable(to_gpu(inputs))\n",
    "        labels = Variable(to_gpu(labels))\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation:\n",
    "    # TODO: spell this out!\n",
    "    valid_inputs, valid_labels = get_data()\n",
    "    \n",
    "    inputs = Variable(to_gpu(valid_inputs))\n",
    "    labels = Variable(to_gpu(valid_labels))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        # spell_out:\n",
    "        save_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "### tensor example\n",
    "x_cpu = torch.randn(10, 20)\n",
    "w_cpu = torch.randn(20, 10)\n",
    "# direct transfer to the GPU\n",
    "x_gpu = x_cpu.cuda()\n",
    "w_gpu = w_cpu.cuda()\n",
    "result_gpu = x_gpu @ w_gpu\n",
    "# get back from GPU to CPU\n",
    "result_cpu = result_gpu.cpu()\n",
    "print(result_cpu)\n",
    "\n",
    "### model example\n",
    "model = model.cuda()\n",
    "# train step\n",
    "inputs = Variable(inputs.cuda())\n",
    "outputs = model(inputs)\n",
    "# get back from GPU to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(10, 20), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 3), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 3), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "for step in range(5):\n",
    "    pred = F.sigmoid(x @ w1)\n",
    "    pred = F.sigmoid(pred @ w2)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # calculate new gradients\n",
    "    loss.backward()\n",
    "    # apply new gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(3, 4), requires_grad=False)\n",
    "y = Variable(torch.randn(3, 2), requires_grad=False)\n",
    "# define some weights\n",
    "w1 = Variable(torch.randn(4, 2), requires_grad=True)\n",
    "w2 = Variable(torch.FloatTensor(w1.data.numpy()), requires_grad=True)\n",
    "for i in range(5):\n",
    "    loss1 = torch.mean((y - x @ w1) ** 2)\n",
    "    loss2 = torch.mean((y - x @ w2) ** 2)\n",
    "# calculate the gradients\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "    print(\"w1 grad(zeroed)\", w1.grad)\n",
    "    print(\"w2 grad(not zeroed)\", w2.grad)\n",
    "    w1.grad.data.zero_()\n",
    "    print('-'*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
